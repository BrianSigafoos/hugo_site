---
title: ML papers
slug: ml-papers
date: 2023-01-27T11:45:51-05:00
summary: A collection of interesting and important machine learning papers
collection_swe_toolbox: true
---

## Reading List

New and interesting papers

- 2023 - [Dreamix: Video Diffusion Models are General Video Editors](https://arxiv.org/abs/2302.01329) - [project page](https://dreamix-video-editing.github.io/)
- 2023 - [simple diffusion: End-to-end diffusion for high resolution images](https://arxiv.org/abs/2301.11093)
- 2023 - [Language Models are Drummers: Drum Composition with Natural Language Pre-Training](https://arxiv.org/abs/2301.01162)

## Stable Diffusion

- 2022 - [Imagic: Text-Based Real Image Editing with Diffusion Models](https://arxiv.org/abs/2210.09276) - [project page](https://imagic-editing.github.io)
- 2022 - [DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://arxiv.org/abs/2208.12242) -  [project page](https://dreambooth.github.io/)
- 2021 - [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752) - introduction of Stable Diffusion
- 2021 - [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
  - 2023 - [Using LoRA for Efficient Stable Diffusion Fine-Tuning](https://huggingface.co/blog/lora)

## Important papers

Most are seminal, introducing something new, and a few are reviews that help summarize.

- 2020 - [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](<https://arxiv.org/abs/2010.11929>) - ViT (Vision Transformer)
- 2020 - [Autoencoders](<https://arxiv.org/abs/2003.05991>) - Summary paper
- 2020 - [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) - "GPT-3"
- 2019 - [Understanding LSTM -- a tutorial into Long Short-Term Memory Recurrent Neural Networks](https://arxiv.org/abs/1909.09586) - LSTM
- 2017 - [Decoupled Weight Decay Regularization](<https://arxiv.org/abs/1711.05101>) - "AdamW"
- 2017 - [Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937) - VQ-VAE, vector autoencoders
- 2017 - [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - introduces the transformer architecture
- 2016 - [WaveNet: A Generative Model for Raw Audio](https://arxiv.org/abs/1609.03499)
- 2016 - [Layer Normalization](https://arxiv.org/abs/1607.06450)
- 2015 - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - "ResNet", introduces "residual blocks" that transform data, then a skip connection from the previous features
- 2014 - [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980) - "Adam"
- 2014 - [Dropout: a simple way to prevent neural networks from overfitting](https://dl.acm.org/doi/abs/10.5555/2627435.2670313)
- 2014 - [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078v3) - "GRU", a Gated Recurrent Unit
  - GRU is a type of recurrent neural network (RNN). It is similar to an LSTM, but only has two gates - a reset gate and an update gate - and notably lacks an output gate. Fewer parameters means GRUs are generally easier/faster to train than their LSTM counterparts
- 2014 - [Generative Adversarial Networks](<https://arxiv.org/abs/1406.2661>) - "GAN"

## Resources

- [Video walkthrough: Progressive Distillation for Fast Sampling of Diffusion Models](https://www.youtube.com/watch?v=ZXuK6IRJlnk)
